[
  
  {
    "title": "PINNs_Inference",
    "url": "/AI4science/posts/pinns/",
    "categories": "Computational sciences",
    "tags": "pinn",
    "date": "2023-11-04 12:00:00 +0800",
    





    
    "snippet": "Trained to solve supervised learning tasks, Physic-informed neural networks (PINNs) are defined for two classes of problems:data-driven solution and data-driven discovery of partial differential eq...",
    "content": "Trained to solve supervised learning tasks, Physic-informed neural networks (PINNs) are defined for two classes of problems:data-driven solution and data-driven discovery of partial differential equations (PDEs).Data-driven solutionGiven a general form of PDEs $f(t,x)$, PINN could get to the solution $u(t,x)$ of this 1-D non-linear equation with only small amount of training data, namely initial and boundary conditions of $u(t,x)$ as well as sampled data of $f(t,x)$.E.g., Discrete time inference equation: Allen-Cahn equation\\[f(t,x)=u_{t}+uu_{xx}-(0.01/\\pi)u_{xx}\\]By minimizing the mean square error loss $MSE_{u}+MSE_{f}$,\\[MSE_{u}=\\frac{1}{N_{u}}\\sum_{i=0}^{N_{u}}\\lvert u(t_{u}^{i},x_{u}^{i})-u^{i} \\rvert^2\\]\\[MSE_{f}=\\frac{1}{N_{f}}\\sum_{i=0}^{N_{f}}\\lvert f(t_{f}^{i},x_{f}^{i}) \\rvert^2\\]To approximate $u(t,x)$ by $f(t,x)$, neural networks are defined as follows:  Weights between layers are defined like xavier_init    def xavier_init(self, size):     in_dim = size[0]     out_dim = size[1]             xavier_stddev = np.sqrt(2/(in_dim + out_dim))     return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)        Given the number of layers, weights and biases are produced by initialize_NN    def initialize_NN(self, layers):             weights = []     biases = []     num_layers = len(layers)      for l in range(0,num_layers-1):         W = self.xavier_init(size=[layers[l], layers[l+1]])         b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)         weights.append(W)         biases.append(b)             return weights, biases        Given weights and biases, a complete neural network without the final activation is designed by neural_net    def neural_net(self, X, weights, biases):     num_layers = len(weights) + 1     H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0     for l in range(0,num_layers-2):         W = weights[l]         b = biases[l]         H = tf.tanh(tf.add(tf.matmul(H, W), b))     W = weights[-1]     b = biases[-1]     Y = tf.add(tf.matmul(H, W), b) # appending tanh?     return Y        where, H seems like restricting the value of the first layer within [-1,1].    As weights, dummy_x0(x1)_tf is identity matirx of x0(x1) rows and q columns, leading to no changes at given case.    def fwd_gradients_0(self, U, x):             g = tf.gradients(U, x, grad_ys=self.dummy_x0_tf)[0]     return tf.gradients(g, self.dummy_x0_tf)[0] def fwd_gradients_1(self, U, x):         g = tf.gradients(U, x, grad_ys=self.dummy_x1_tf)[0] return tf.gradients(g, self.dummy_x1_tf)[0]         Through net_U0 and net_U1, $f(t,x)$ is generated by $U$ and $U_{xx}$, with the same dimension of $N$ x $q$, while $U1$ is generated  with a dimension of (N, q+1).    def net_U0(self, x): # x0     U1 = self.neural_net(x, self.weights, self.biases)     U = U1[:,:-1]     U_x = self.fwd_gradients_0(U, x)     U_xx = self.fwd_gradients_0(U_x, x)     F = 5.0*U - 5.0*U**3 + 0.0001*U_xx     U0 = U1 - self.dt*tf.matmul(F, self.IRK_weights.T) # (N, q+1)-(N, q)=(N,q+1)     return U0 def net_U1(self, x): # x1     U1 = self.neural_net(x, self.weights, self.biases)     U1_x = self.fwd_gradients_1(U1, x)     return U1, U1_x   #（2, q+1)          loss is defined by $u0$, products of functions of $net_{U0}$ and $net_{U1}$ in step 5.    self.U0_pred = self.net_U0(self.x0_tf) # N x (q+1)self.U1_pred, self.U1_x_pred= self.net_U1(self.x1_tf) # N1 x (q+1)self.loss = tf.reduce_sum(tf.square(self.u0_tf - self.U0_pred)) + \\               tf.reduce_sum(tf.square(self.U1_pred[0,:] - self.U1_pred[1,:])) + \\            tf.reduce_sum(tf.square(self.U1_x_pred[0,:] - self.U1_x_pred[1,:]))          # (N,1)-(N,q+1); (1,q+1)-(1,q+1); (1,q+1)-(1,q+1)        The first row of self.loss represents $MSE_{f}$;The second and third rows of self.loss represent $MSE_{u}$, with the initial and boundary data, $x1$.    input data:          $x0$ is resampled x, dimension (N x 1).      $u0$ randomly transposed row of u(t,x), results of specific t, with some noises (0 in Burger’s equation), dimension(N x 1)      $x1$ is vertically stacked data [-1,1].      $dt$ is the random length of t, $t1-t0$, where $t0$ correponds to the position of $u0$.      layers is a list, representing the strusture of $f(t,x)$.      $q+1$ is the last element in layers, the target size at bounary condition, 100+1 samples.        Via net_U1 function, a (N, q+1) matrix is predicted by inputing a certain value of x* (N,1) in the $U1_{pred}$, which is the target of PINNs, with error calculated by    error = np.linalg.norm(U1_pred[:,-1] - Exact[idx_t1,:], 2)/np.linalg.norm(Exact[idx_t1,:], 2)      Questiona are,  why the neurons of the last layer is firstly set as (q+1), and then clip the dimension of $U$ to be (N, q), instead of initially settled down as (N, q) in step 5?  the predicted matrix [:,:-1] (N,q) represents the distribution of solution $u(t,x)?  In training, $net_{U1}$ is put $x1$ with a dimension of (2,1); while in testing, $net_{U1}$ is put x* with a dimension of (N,1). Why the dimension of input in U1_pred is different?Changes in versions of tensorflow  tf.compat.v1 should be added in front of some old v1 functions in v2 version of tensorflow.  L-BFGS-B method in tf.opt.ScipyOptimizerInterface could be replaced by tensorflow_probability.optimizer.lbfgs_minimize()Explanations in programming  tf.placeholder() generates a map, different from tf. Varaibles() and tf.Tensor(). This type is hashable, while the other two are not.          placeholder() with a new name .._tf in tf_dict.keys() is irreplaceable.        tf.compat.v1.dsable_eager_execution() couldn’t be used, as in resources_variable_ops.py script, self.numpy() wouldn’t be implemented.          To avoid loss in Adam shoule be a function in eager execution, if the disable function is used, then tensor.ref() is useless. As a result, tf.compat.v1.placeholder() couldn’t be replaced by tenspr.ref()      As a result, tf.gradients is taken place by tf.GradientTape(), which requires targets to be a list or nested tensors. In the case of deep learning, the forward function should be appplied within GradientTape().        Why both Adam and L-BFGS optimizer were used.          With only Adam, the error calculated by $np.linalg.norm$ between $U1_pred[:,-1]$ (512,1) and $x$ (1, 512) is 1.37,      With both Adam and L-BFGS, the error is        The effect of appending tanh in neural_net function.  Effects of removing dummy_x0(x1)          They are removed in tf.GradientTape()      Difference between discrete time inference and continuous onePersonal views  In th code of PINNs, it focues on the variation of x with a specific random value of t  The innovation lies in the treatment of loss function  What’s the difference between PINNs and adding physics constraints into loss function1 ?  Code is updated and simplified here            Chen H, Huang J J, Dash S S, et al. A hybrid deep learning framework with physical process description for simulation of evapotranspiration[J]. Journal of Hydrology, 2022, 606: 127422. &#8617;      "
  },
  
  {
    "title": "Hybrid hydrologcial models",
    "url": "/AI4science/posts/h2m/",
    "categories": "models",
    "tags": "hybrid models",
    "date": "2023-10-30 00:00:00 +0800",
    





    
    "snippet": "Concepts in H2MParameters in hybrid models are categorized into ML parameters and physical parameters1. Thus, hybrid modeling are grouped into parameteric (both learned) and non-parameteric (the la...",
    "content": "Concepts in H2MParameters in hybrid models are categorized into ML parameters and physical parameters1. Thus, hybrid modeling are grouped into parameteric (both learned) and non-parameteric (the latter parameters fixed) ones.H2M structureThe end-to-end hybrid hydrological modelCreative points  A physical state $s_{t-1}$ is taken into the recurrent neural network $ g_{RNN} $ for accounting memory effects.\\[h_{t}=g_{RNN}(h_{t-1},[x_{t},s_{t-1}])\\]      A mapping output function $g_{out}$ that links certain latent variable or coefficient $p_{t}$ in process-based models with $h_{t}$        The process-based model $f_{pb}$ considers not only original physical constraints $s_{t}$, but also memory effects from RNN.  \\[y_{t}, s_{t}=f_{pb}(p_{t},x_{t},s_{t-1})\\]Comparisons with others  H2M          Pros                  hard physical constraints          additional insights of latent variables and coefficients          partial interpretability                    Cons                  qunatification of uncertainties          Generalizability needs to be investigated in certain cases                      Regularization via loss functions (soft constriants)          penalizing physically inconsistent results      Pros                  additional means in diagosing physical inconsistency                    Cons                  penalizing physically inconsistent results does not always make sense          lacks of hard physical constraints          no insights via latent variables and coefficients                      Mass conserving neural networks (hard constraints)          adding inductive biase      Pros                  enhances robustness and generalizability          good performance in extreme events                    Cons                  does not outperform non-mass conserving architecture          limited interpretability                      Data assimilation          combining simulations from process-based models and obervation for the optimal estimates of geophysical states      Pros                  aim to quantify errors                    Cons                  remaining model errors                    Personal Opinions  Compared to physics-constrained machine learning2, using neural networks to replace empirical parameter, it steps further in linking RNNs with latent variables (coefficients) in process-based models, which further taps the potential of AI.  A question is why acknowledge latent variables (coefficients) activated by softplus function after LSTM layer as Evapotranspration and snow water equvilatent in multi-task layerhttps://ieeexplore.ieee.org/document/8578879, and how to figure out its effect.Footnote            Deep learning and hybrid modeling of global vegetation and hydrology. Basil Kraft. 2022. &#8617;              Zhao, W. L., Gentine, P., Reichstein, M., Zhang, Y., Zhou, S., Wen, Y., et al. (2019). Physics-constrained machine learning of evapotranspiration. Geophysical Research Letters, 46, 14496–14507. https://doi.org/10.1029/2019GL085291 &#8617;      "
  },
  
  {
    "title": "Preface",
    "url": "/AI4science/posts/categories/",
    "categories": "ReadMe",
    "tags": "hybrid models, scientific machine learning, pinn, causal deep learning",
    "date": "2023-10-29 00:00:00 +0800",
    





    
    "snippet": "Great progresses have been made by researchers for the mutual devleopment between physically-based models (mechanism models) and Artificial intelligence (AI for short). Several typical milestones r...",
    "content": "Great progresses have been made by researchers for the mutual devleopment between physically-based models (mechanism models) and Artificial intelligence (AI for short). Several typical milestones represent different genres, while reaching out the same purpose. To clarify each of them, from personal perspectives, groups are roughly made for better understanding of tags and categories in the following blogs.Scientific machine learningThree separate aspects of scientific benchmarking that apply in the context of ML benchmarks for science, namely, scientific ML benchmarking, application benchmarking and system benchmarking1.  Scientific ML benchmarking. This is concerned with algorithmic improvements that help reach the scientific targets specified for a given dataset.                            SciNet2: discvering simple physical concepts between distances and angles in the earth-moon system.                                      Physics-informed neural networks3: solve and discover low-dimension nonlinear partial differential equations.                    Deep operator networks:        Application benchmarking. This aspect of ML benchmarks is concerned with exploring the performance of the complete ML application.          hybrid models4: combineing concepts from machine learning and physically-based models, by parameterizing or replacingpart of mechanisms with machine learning, in order to entitle ML with interpretability.                                            hybrid hydrological model5                                          causal deep learning6:        System benchmarking. This is concerned with investigating performance effects of the system hardware architecture on improving the scientific outcomes/targets.Reference            Scientific machine learning benchmarks. (2022). https://doi.org/10.1038/s42254-022-00441-7. &#8617;              Discovering physical concepts with neural networks. (2020). https://doi.org/10.48550/arXiv.1807.10300. &#8617;              Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. (2019). https://doi.org/10.1016/j.jcp.2018.10.045. &#8617;              Deep Learning and Process Understanding for Data-Driven Earth System Science. (2019). https://doi.org/10.1038/s41586-019-0912-1. &#8617;              Hybrid modeling: fusion of a deep learning approach and a physics-based model for global hydrological modeling. (2020). https://doi.org/10.5194/isprs-archives-XLIII-B2-2020-1537-2020. &#8617;              Causal deep learning. (2023). https://arxiv.org/abs/2303.02186. &#8617;      "
  }
  
]

